{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back_Prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"Represents a scalar value and its gradient.\"\"\"\n",
    "    def __init__(self, data, _children=(), _op=''):\n",
    "        self.data = data  # The actual scalar value\n",
    "        self.grad = 0  # Gradient for backpropagation\n",
    "        self._backward = lambda: None  # Backward function for autograd\n",
    "        self._prev = set(_children)  # Set of child nodes (dependencies)\n",
    "        self._op = _op  # Operation that produced this node\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\"\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data + other.data, (self, other), '+')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += out.grad\n",
    "            other.grad += out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Value) else Value(other)\n",
    "        out = Value(self.data * other.data, (self, other), '*')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def relu(self):\n",
    "        out = Value(self.data if self.data > 0 else 0, (self,), 'ReLU')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (out.data > 0) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def tanh(self):\n",
    "        t = (2 / (1 + (-2 * self.data).exp())) - 1\n",
    "        out = Value(t, (self,), 'tanh')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (1 - t ** 2) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __pow__(self, other):\n",
    "        assert isinstance(other, (int, float))\n",
    "        out = Value(self.data ** other, (self,), f'**{other}')\n",
    "\n",
    "        def _backward():\n",
    "            self.grad += (other * self.data ** (other - 1)) * out.grad\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * other ** -1\n",
    "\n",
    "    def backward(self):\n",
    "        topo = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_topo(v):\n",
    "            if v not in visited:\n",
    "                visited.add(v)\n",
    "                for child in v._prev:\n",
    "                    build_topo(child)\n",
    "                topo.append(v)\n",
    "\n",
    "        build_topo(self)\n",
    "\n",
    "        self.grad = 1\n",
    "        for node in reversed(topo):\n",
    "            node._backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset for XNOR operation with 3 variables\n",
    "def create_xnor_data():\n",
    "    # Training data (12 samples)\n",
    "    train_X = [\n",
    "        [0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1],\n",
    "        [1, 0, 0], [1, 0, 1], [1, 1, 1], [1, 1, 0]\n",
    "    ]\n",
    "    train_y = [1, 0, 0, 1, 0, 1, 0, 1]  # XNOR outputs\n",
    "\n",
    "    # Test data (4 samples)\n",
    "    test_X = [[1, 0, 1], [1, 1, 1], [1, 1, 0]]\n",
    "    test_y = [1, 0, 1]\n",
    "\n",
    "    return train_X, train_y, test_X, test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and biases\n",
    "def init_weights(input_size, hidden_sizes, output_size):\n",
    "    \"\"\"Initialize weights and biases for the network.\"\"\"\n",
    "    sizes = [input_size] + hidden_sizes + [output_size]  # Layer sizes\n",
    "    weights = []\n",
    "    biases = []\n",
    "    for i in range(len(sizes) - 1):\n",
    "        # Initialize weights and biases for each layer\n",
    "        layer_weights = [[Value(random.uniform(-1, 1)) for _ in range(sizes[i + 1])] for _ in range(sizes[i])]\n",
    "        layer_biases = [Value(random.uniform(-1, 1)) for _ in range(sizes[i + 1])]\n",
    "        weights.append(layer_weights)\n",
    "        biases.append(layer_biases)\n",
    "    return weights, biases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_layer(inputs, weights, biases, activation=\"relu\"):\n",
    "    \"\"\"Forward pass for a single layer.\"\"\"\n",
    "    outputs = []\n",
    "    for j in range(len(weights[0])):  # Loop through each neuron in the layer\n",
    "        # Compute weighted sum + bias\n",
    "        z = sum((inputs[i] * weights[i][j] for i in range(len(inputs))), Value(0)) + biases[j]\n",
    "        # Apply activation function\n",
    "        if activation == \"relu\":\n",
    "            outputs.append(z.relu())\n",
    "        elif activation == \"sigmoid\":\n",
    "            outputs.append(z.tanh())\n",
    "        else:\n",
    "            outputs.append(z)  # No activation\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward pass through the network\n",
    "def forward_pass(X, weights, biases):\n",
    "    \"\"\"Compute forward pass through the entire network.\"\"\"\n",
    "    inputs = [Value(x) for x in X]  # Convert inputs to Value objects\n",
    "    layer1 = forward_layer(inputs, weights[0], biases[0], activation=\"relu\")\n",
    "    layer2 = forward_layer(layer1, weights[1], biases[1], activation=\"relu\")\n",
    "    layer3 = forward_layer(layer2, weights[2], biases[2], activation=\"relu\")\n",
    "    output = forward_layer(layer3, weights[3], biases[3], activation=\"tanh\")  # Output layer\n",
    "    return output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_network(train_X, train_y, weights, biases, epochs=100000, lr=0.001):\n",
    "    \"\"\"Train the network using the training dataset.\"\"\"\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for X, y in zip(train_X, train_y):\n",
    "            # Forward pass\n",
    "            pred = forward_pass(X, weights, biases)\n",
    "\n",
    "            # Calculate loss (Mean Squared Error)\n",
    "            target = Value(y)\n",
    "            loss = (pred - target) ** 2\n",
    "            total_loss += loss.data\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "\n",
    "            # Update weights and biases\n",
    "            for layer_weights, layer_biases in zip(weights, biases):\n",
    "                for i in range(len(layer_weights)):\n",
    "                    for j in range(len(layer_weights[i])):\n",
    "                        layer_weights[i][j].data -= lr * layer_weights[i][j].grad  # Gradient descent\n",
    "                        layer_weights[i][j].grad = 0  # Reset gradient\n",
    "                for j in range(len(layer_biases)):\n",
    "                    layer_biases[j].data -= lr * layer_biases[j].grad  # Gradient descent\n",
    "                    layer_biases[j].grad = 0  # Reset gradient\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {total_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the network\n",
    "def test_network(test_X, test_y, weights, biases):\n",
    "    \"\"\"Test the network using the test dataset.\"\"\"\n",
    "    print(\"\\nTesting the network:\")\n",
    "    for X, y in zip(test_X, test_y):\n",
    "        pred = forward_pass(X, weights, biases)\n",
    "        print(f\"Input: {X}, Prediction: {round(pred.data)}, Actual: {y}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 21.288975705647882\n",
      "Epoch 100, Loss: 2.0789132578239635\n",
      "Epoch 200, Loss: 2.0105986246684417\n",
      "Epoch 300, Loss: 2.0084255679300416\n",
      "Epoch 400, Loss: 2.007507200932536\n",
      "Epoch 500, Loss: 2.0069668073251163\n",
      "Epoch 600, Loss: 2.006538438993442\n",
      "Epoch 700, Loss: 2.006184499761303\n",
      "Epoch 800, Loss: 2.005876161807622\n",
      "Epoch 900, Loss: 2.0056201116431667\n",
      "\n",
      "Testing the network:\n",
      "Input: [1, 0, 1], Prediction: 0, Actual: 1\n",
      "Input: [1, 1, 1], Prediction: 1, Actual: 0\n",
      "Input: [1, 1, 0], Prediction: 0, Actual: 1\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "def main():\n",
    "    train_X, train_y, test_X, test_y = create_xnor_data()  # Create dataset\n",
    "    weights, biases = init_weights(3, [5, 5, 5], 1)  # 3 input nodes, 3 hidden layers with 5 nodes each, 1 output node\n",
    "\n",
    "    train_network(train_X, train_y, weights, biases, epochs=1000, lr=0.001)  # Train the network\n",
    "    test_network(test_X, test_y, weights, biases)  # Test the network\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2805\n",
      "Epoch 500, Loss: 0.1421\n",
      "Epoch 1000, Loss: 0.0501\n",
      "Epoch 1500, Loss: 0.0138\n",
      "Epoch 2000, Loss: 0.0047\n",
      "Epoch 2500, Loss: 0.0021\n",
      "Epoch 3000, Loss: 0.0011\n",
      "Epoch 3500, Loss: 0.0007\n",
      "Epoch 4000, Loss: 0.0004\n",
      "Epoch 4500, Loss: 0.0003\n",
      "\n",
      "Predictions after training:\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# XNOR dataset\n",
    "X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "              [1, 0, 0], [1, 0, 1], [1, 1, 1], [1, 1, 0]])\n",
    "y = np.array([[1], [0], [0], [1], [0], [1], [0], [1]])  \n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = 3\n",
    "hidden_size = 5  \n",
    "output_size = 1\n",
    "\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size)  \n",
    "b1 = np.zeros((1, hidden_size))  \n",
    "W2 = np.random.randn(hidden_size, output_size)  \n",
    "b2 = np.zeros((1, output_size))  \n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward pass\n",
    "def forward(X):\n",
    "    global z1, a1, z2, y_pred\n",
    "    z1 = np.dot(X, W1) + b1  # Hidden layer computation\n",
    "    a1 = relu(z1)  # Apply ReLU\n",
    "    z2 = np.dot(a1, W2) + b2  # Output layer computation\n",
    "    y_pred = sigmoid(z2)  # Apply Sigmoid\n",
    "    return y_pred\n",
    "\n",
    "# Backward pass (Gradient Descent)\n",
    "def backward(X, y, lr=0.01):\n",
    "    global W1, b1, W2, b2\n",
    "    m = len(y)\n",
    "\n",
    "    # Compute gradients\n",
    "    dz2 = y_pred - y  # Derivative of loss w.r.t. output\n",
    "    dW2 = (1 / m) * np.dot(a1.T, dz2)  # Gradient for W2\n",
    "    db2 = (1 / m) * np.sum(dz2, axis=0, keepdims=True)  # Gradient for b2\n",
    "\n",
    "    dz1 = np.dot(dz2, W2.T) * (z1 > 0)  # ReLU derivative\n",
    "    dW1 = (1 / m) * np.dot(X.T, dz1)  # Gradient for W1\n",
    "    db1 = (1 / m) * np.sum(dz1, axis=0, keepdims=True)  # Gradient for b1\n",
    "\n",
    "    # Update parameters\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    forward(X)\n",
    "    backward(X, y, lr=0.1)\n",
    "    if epoch % 500 == 0:\n",
    "        loss = np.mean((y_pred - y) ** 2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nPredictions after training:\")\n",
    "print(y_pred.round())  # Round to 0 or 1 for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.2805\n",
      "Epoch 500, Loss: 0.1421\n",
      "Epoch 1000, Loss: 0.0501\n",
      "Epoch 1500, Loss: 0.0138\n",
      "Epoch 2000, Loss: 0.0047\n",
      "Epoch 2500, Loss: 0.0021\n",
      "Epoch 3000, Loss: 0.0011\n",
      "Epoch 3500, Loss: 0.0007\n",
      "Epoch 4000, Loss: 0.0004\n",
      "Epoch 4500, Loss: 0.0003\n",
      "\n",
      "Predictions after training:\n",
      "[[1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]]\n",
      "Input: [0 0 0], Predicted Output: [1.], Actual Output: [1]\n",
      "Input: [0 0 1], Predicted Output: [0.], Actual Output: [0]\n",
      "Input: [0 1 0], Predicted Output: [0.], Actual Output: [0]\n",
      "Input: [0 1 1], Predicted Output: [1.], Actual Output: [1]\n",
      "Input: [1 0 0], Predicted Output: [0.], Actual Output: [0]\n",
      "Input: [1 0 1], Predicted Output: [1.], Actual Output: [1]\n",
      "Input: [1 1 1], Predicted Output: [0.], Actual Output: [0]\n",
      "Input: [1 1 0], Predicted Output: [1.], Actual Output: [1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# XNOR dataset\n",
    "X = np.array([[0, 0, 0], [0, 0, 1], [0, 1, 0], [0, 1, 1], \n",
    "              [1, 0, 0], [1, 0, 1], [1, 1, 1], [1, 1, 0]])\n",
    "y = np.array([[1], [0], [0], [1], [0], [1], [0], [1]])  # XNOR output\n",
    "\n",
    "# Initialize weights and biases\n",
    "input_size = 3\n",
    "hidden_size = 5  # One hidden layer with 5 neurons\n",
    "output_size = 1\n",
    "\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_size, hidden_size)  # Weights from input to hidden\n",
    "b1 = np.zeros((1, hidden_size))  # Bias for hidden layer\n",
    "W2 = np.random.randn(hidden_size, output_size)  # Weights from hidden to output\n",
    "b2 = np.zeros((1, output_size))  # Bias for output layer\n",
    "\n",
    "# Activation functions\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Forward pass\n",
    "def forward(X):\n",
    "    global z1, a1, z2, y_pred\n",
    "    z1 = np.dot(X, W1) + b1  # Hidden layer computation\n",
    "    a1 = relu(z1)  # Apply ReLU\n",
    "    z2 = np.dot(a1, W2) + b2  # Output layer computation\n",
    "    y_pred = sigmoid(z2)  # Apply Sigmoid\n",
    "    return y_pred\n",
    "\n",
    "# Backward pass (Gradient Descent)\n",
    "def backward(X, y, lr=0.01):\n",
    "    global W1, b1, W2, b2\n",
    "    m = len(y)\n",
    "\n",
    "    # Compute gradients\n",
    "    dz2 = y_pred - y  # Derivative of loss w.r.t. output\n",
    "    dW2 = (1 / m) * np.dot(a1.T, dz2)  # Gradient for W2\n",
    "    db2 = (1 / m) * np.sum(dz2, axis=0, keepdims=True)  # Gradient for b2\n",
    "\n",
    "    dz1 = np.dot(dz2, W2.T) * (z1 > 0)  # ReLU derivative\n",
    "    dW1 = (1 / m) * np.dot(X.T, dz1)  # Gradient for W1\n",
    "    db1 = (1 / m) * np.sum(dz1, axis=0, keepdims=True)  # Gradient for b1\n",
    "\n",
    "    # Update parameters\n",
    "    W1 -= lr * dW1\n",
    "    b1 -= lr * db1\n",
    "    W2 -= lr * dW2\n",
    "    b2 -= lr * db2\n",
    "\n",
    "# Training loop\n",
    "epochs = 5000\n",
    "for epoch in range(epochs):\n",
    "    forward(X)\n",
    "    backward(X, y, lr=0.1)\n",
    "    if epoch % 500 == 0:\n",
    "        loss = np.mean((y_pred - y) ** 2)\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "# Test predictions\n",
    "print(\"\\nPredictions after training:\")\n",
    "print(y_pred.round())  # Round to 0 or 1 for binary classification\n",
    "\n",
    "# Define outputs\n",
    "outputs = []\n",
    "for i in range(len(X)):\n",
    "    outputs.append({\n",
    "        \"Input\": X[i],\n",
    "        \"Predicted Output\": y_pred[i].round(),  # Rounded prediction\n",
    "        \"Actual Output\": y[i]\n",
    "    })\n",
    "\n",
    "# Display outputs\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"Input: {output['Input']}, Predicted Output: {output['Predicted Output']}, Actual Output: {output['Actual Output']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "class Neuron():\n",
    "    '''\n",
    "        A conceptual Neuron hat can be trained using a \n",
    "        fit and predict methodology, without any library\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, position_in_layer, is_output_neuron=False):\n",
    "        self.weights = []\n",
    "        self.inputs = []\n",
    "        self.output = None\n",
    "        \n",
    "        # This is used for the backpropagation update\n",
    "        self.updated_weights = []\n",
    "        # This is used to know how to update the weights\n",
    "        self.is_output_neuron = is_output_neuron\n",
    "        # This delta is used for the update at the backpropagation\n",
    "        self.delta = None\n",
    "        # This is used for the backpropagation update\n",
    "        self.position_in_layer = position_in_layer \n",
    "        \n",
    "    def attach_to_output(self, neurons):\n",
    "        '''\n",
    "            Helper function to store the reference of the other neurons\n",
    "            To this particular neuron (used for backpropagation)\n",
    "        '''\n",
    "        \n",
    "        self.output_neurons = neurons\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "            simple sigmoid function (logistic) used for the activation\n",
    "        '''\n",
    "        return 1 / (1 + math.exp(-x))\n",
    "    \n",
    "    def init_weights(self, num_input):\n",
    "        '''\n",
    "            This is used to setup the weights when we know how many inputs there is for\n",
    "            a given neuron\n",
    "        '''\n",
    "        \n",
    "        # Randomly initalize the weights\n",
    "        for i in range(num_input+1):\n",
    "            self.weights.append(random.uniform(0,1))\n",
    "        \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Given a row of data it will predict what the output should be for\n",
    "            this given neuron. We can have many input, but only one output for a neuron\n",
    "        '''\n",
    "        \n",
    "        # Reset the inputs\n",
    "        self.inputs = []\n",
    "        \n",
    "        # We iterate over the weights and the features in the given row\n",
    "        activation = 0\n",
    "        for weight, feature in zip(self.weights, row):\n",
    "            self.inputs.append(feature)\n",
    "            activation = activation + weight*feature\n",
    "            \n",
    "        \n",
    "        self.output = self.sigmoid(activation)\n",
    "        return self.output\n",
    "    \n",
    "        \n",
    "            \n",
    "    def update_neuron(self):\n",
    "        '''\n",
    "            Will update a given neuron weights by replacing the current weights\n",
    "            with those used during the backpropagation. This need to be done at the end of the\n",
    "            backpropagation\n",
    "        '''\n",
    "        \n",
    "        self.weights = []\n",
    "        for new_weight in self.updated_weights:\n",
    "            self.weights.append(new_weight)\n",
    "    \n",
    "    def calculate_update(self, learning_rate, target):\n",
    "        '''\n",
    "            This function will calculate the updated weights for this neuron. It will first calculate\n",
    "            the right delta (depending if this neuron is a ouput or a hidden neuron), then it will\n",
    "            calculate the right updated_weights. It will not overwrite the weights yet as they are needed\n",
    "            for other update in the backpropagation algorithm.\n",
    "        '''\n",
    "        \n",
    "        if self.is_output_neuron:\n",
    "            # Calculate the delta for the output\n",
    "            self.delta = (self.output - target)*self.output*(1-self.output)\n",
    "        else:\n",
    "            # Calculate the delta\n",
    "            delta_sum = 0\n",
    "            # this is to know which weights this neuron is contributing in the output layer\n",
    "            cur_weight_index = self.position_in_layer \n",
    "            for output_neuron in self.output_neurons:\n",
    "                delta_sum = delta_sum + (output_neuron.delta * output_neuron.weights[cur_weight_index])\n",
    "\n",
    "            # Update this neuron delta\n",
    "            self.delta = delta_sum*self.output*(1-self.output)\n",
    "            \n",
    "            \n",
    "        # Reset the update weights\n",
    "        self.updated_weights = []\n",
    "        \n",
    "        # Iterate over each weight and update them\n",
    "        for cur_weight, cur_input in zip(self.weights, self.inputs):\n",
    "            gradient = self.delta*cur_input\n",
    "            new_weight = cur_weight - learning_rate*gradient\n",
    "            self.updated_weights.append(new_weight)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer():\n",
    "    '''\n",
    "        Layer is modelizing a layer in the fully-connected-feedforward neural network architecture.\n",
    "        It will play the role of connecting everything together inside and will be doing the backpropagation \n",
    "        update.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, num_neuron, is_output_layer = False):\n",
    "        \n",
    "        # Will create that much neurons in this layer\n",
    "        self.is_output_layer = is_output_layer\n",
    "        self.neurons = []\n",
    "        for i in range(num_neuron):\n",
    "            # Create neuron\n",
    "            neuron = Neuron(i,  is_output_neuron=is_output_layer)\n",
    "            self.neurons.append(neuron)\n",
    "    \n",
    "    def attach(self, layer):\n",
    "        '''\n",
    "            This function attach the neurons from this layer to another one\n",
    "            This is needed for the backpropagation algorithm\n",
    "        '''\n",
    "        # Iterate over the neurons in the current layer and attach \n",
    "        # them to the next layer\n",
    "        for in_neuron in self.neurons:\n",
    "            in_neuron.attach_to_output(layer.neurons)\n",
    "            \n",
    "    def init_layer(self, num_input):\n",
    "        '''\n",
    "            This will initialize the weights of each neuron in the layer.\n",
    "            By giving the right num_input it will spawn the right number of weights\n",
    "        '''\n",
    "        \n",
    "        # Iterate over each of the neuron and initialize\n",
    "        # the weights that connect with the previous layer\n",
    "        for neuron in self.neurons:\n",
    "            neuron.init_weights(num_input)\n",
    "    \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            This will calcualte the activations for the full layer given the row of data \n",
    "            streaming in.\n",
    "        '''\n",
    "        row.append(1) # need to add the bias\n",
    "        activations = [neuron.predict(row) for neuron in self.neurons]\n",
    "        return activations\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerPerceptron():\n",
    "    '''\n",
    "        We will be creating the multi-layer perceptron with only two layer:\n",
    "        an input layer, a perceptrons layer and a one neuron output layer which does binary classification\n",
    "    '''\n",
    "    def __init__(self, learning_rate = 0.01, num_iteration = 100):\n",
    "        \n",
    "        # Layers\n",
    "        self.layers = []\n",
    "                \n",
    "        # Training parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iteration = num_iteration\n",
    "        \n",
    "        \n",
    "    def add_output_layer(self, num_neuron):\n",
    "        '''\n",
    "            This helper function will create a new output layer and add it to the architecture\n",
    "        '''\n",
    "        self.layers.insert(0, Layer(num_neuron, is_output_layer = True))\n",
    "    \n",
    "    def add_hidden_layer(self, num_neuron):\n",
    "        '''\n",
    "            This helper function will create a new hidden layer, add it to the architecture\n",
    "            and finally attach it to the front of the architecture\n",
    "        '''\n",
    "        # Create an hidden layer\n",
    "        hidden_layer = Layer(num_neuron)\n",
    "        # Attach the last added layer to this new layer\n",
    "        hidden_layer.attach(self.layers[0])\n",
    "        # Add this layers to the architecture\n",
    "        self.layers.insert(0, hidden_layer)\n",
    "\n",
    "    def add_input_layer(self, num_neuron):\n",
    "        '''\n",
    "            This helper function will create a starting input layer, add it to the architecture\n",
    "            and finally attach it to the front of the architecture\n",
    "        '''\n",
    "        # Create an input layer\n",
    "        hidden_layer = Layer(num_neuron)\n",
    "        # Attach the last added layer to this new layer\n",
    "        hidden_layer.attach(self.layers[0])\n",
    "        # Add this layers to the architecture\n",
    "        self.layers.insert(0, hidden_layer)\n",
    "        \n",
    "    def update_layers(self, target):\n",
    "        '''\n",
    "            Will update all the layers by calculating the updated weights and then updating \n",
    "            the weights all at once when the new weights are found.\n",
    "        '''\n",
    "        # Iterate over each of the layer in reverse order\n",
    "        # to calculate the updated weights\n",
    "        for layer in reversed(self.layers):\n",
    "                           \n",
    "            # Calculate update the hidden layer\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.calculate_update(self.learning_rate, target)  \n",
    "        \n",
    "        # Iterate over each of the layer in normal order\n",
    "        # to update the weights\n",
    "        for layer in self.layers:\n",
    "            for neuron in layer.neurons:\n",
    "                neuron.update_neuron()\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "            Main training function of the neural network algorithm. This will make use of backpropagation.\n",
    "            It will use stochastic gradient descent by selecting one row at random from the dataset and \n",
    "            use predict to calculate the error. The error will then be backpropagated and new weights calculated.\n",
    "            Once all the new weights are calculated, the whole network weights will be updated\n",
    "        '''\n",
    "        num_row = len(X)\n",
    "        num_feature = len(X[0]) # Here we assume that we have a rectangular matrix\n",
    "        \n",
    "        # Init the weights throughout each of the layer\n",
    "        self.layers[0].init_layer(num_feature)\n",
    "        \n",
    "        for i in range(1, len(self.layers)):\n",
    "            num_input = len(self.layers[i-1].neurons)\n",
    "            self.layers[i].init_layer(num_input)\n",
    "\n",
    "        # Launch the training algorithm\n",
    "        for i in range(self.num_iteration):\n",
    "            \n",
    "            # Stochastic Gradient Descent\n",
    "            r_i = random.randint(0,num_row-1)\n",
    "            row = X[r_i] # take the random sample from the dataset\n",
    "            yhat = self.predict(row)\n",
    "            target = y[r_i]\n",
    "            \n",
    "            # Update the layers using backpropagation   \n",
    "            self.update_layers(target)\n",
    "            \n",
    "            # At every 100 iteration we calculate the error\n",
    "            # on the whole training set\n",
    "            if i % 1000 == 0:\n",
    "                total_error = 0\n",
    "                for r_i in range(num_row):\n",
    "                    row = X[r_i]\n",
    "                    yhat = self.predict(row)\n",
    "                    error = (y[r_i] - yhat)\n",
    "                    total_error = total_error + error**2\n",
    "                mean_error = total_error/num_row\n",
    "                print(f\"Iteration {i} with error = {mean_error}\")\n",
    "        \n",
    "    \n",
    "    def predict(self, row):\n",
    "        '''\n",
    "            Prediction function that will take a row of input and give back the output\n",
    "            of the whole neural network.\n",
    "        '''\n",
    "        \n",
    "        # Gather all the activation in the hidden layer\n",
    "        \n",
    "        activations = self.layers[0].predict(row)\n",
    "        for i in range(1, len(self.layers)):\n",
    "            activations = self.layers[i].predict(activations)\n",
    "\n",
    "        outputs = []\n",
    "        for activation in activations:                        \n",
    "            # Decide if we output a 1 or 0\n",
    "            if activation >= 0.5:\n",
    "                outputs.append(1.0)\n",
    "            else:\n",
    "                outputs.append(0.0)\n",
    "                           \n",
    "        # We currently have only One output allowed\n",
    "        return outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 with error = 0.5\n",
      "Iteration 1000 with error = 0.5\n",
      "Iteration 2000 with error = 0.5\n",
      "Iteration 3000 with error = 0.5\n",
      "Iteration 4000 with error = 0.5\n",
      "Iteration 5000 with error = 0.5\n",
      "Iteration 6000 with error = 0.5\n",
      "Iteration 7000 with error = 0.5\n",
      "Iteration 8000 with error = 0.5\n",
      "Iteration 9000 with error = 0.5\n",
      "Iteration 10000 with error = 0.5\n",
      "Iteration 11000 with error = 0.5\n",
      "Iteration 12000 with error = 0.5\n",
      "Iteration 13000 with error = 0.5\n",
      "Iteration 14000 with error = 0.5\n",
      "Iteration 15000 with error = 0.5\n",
      "Iteration 16000 with error = 0.5\n",
      "Iteration 17000 with error = 0.5\n",
      "Iteration 18000 with error = 0.5\n",
      "Iteration 19000 with error = 0.5\n",
      "Iteration 20000 with error = 0.5\n",
      "Iteration 21000 with error = 0.5\n",
      "Iteration 22000 with error = 0.25\n",
      "Iteration 23000 with error = 0.5\n",
      "Iteration 24000 with error = 0.5\n",
      "Iteration 25000 with error = 0.5\n",
      "Iteration 26000 with error = 0.5\n",
      "Iteration 27000 with error = 0.5\n",
      "Iteration 28000 with error = 0.25\n",
      "Iteration 29000 with error = 0.5\n",
      "Iteration 30000 with error = 0.5\n",
      "Iteration 31000 with error = 0.5\n",
      "Iteration 32000 with error = 0.75\n",
      "Iteration 33000 with error = 0.5\n",
      "Iteration 34000 with error = 0.5\n",
      "Iteration 35000 with error = 0.25\n",
      "Iteration 36000 with error = 0.5\n",
      "Iteration 37000 with error = 0.5\n",
      "Iteration 38000 with error = 0.5\n",
      "Iteration 39000 with error = 0.5\n",
      "Iteration 40000 with error = 0.5\n",
      "Iteration 41000 with error = 0.5\n",
      "Iteration 42000 with error = 0.5\n",
      "Iteration 43000 with error = 0.5\n",
      "Iteration 44000 with error = 0.5\n",
      "Iteration 45000 with error = 0.75\n",
      "Iteration 46000 with error = 0.25\n",
      "Iteration 47000 with error = 0.25\n",
      "Iteration 48000 with error = 0.25\n",
      "Iteration 49000 with error = 0.25\n",
      "Iteration 50000 with error = 0.25\n",
      "Iteration 51000 with error = 0.25\n",
      "Iteration 52000 with error = 0.25\n",
      "Iteration 53000 with error = 0.25\n",
      "Iteration 54000 with error = 0.0\n",
      "Iteration 55000 with error = 0.0\n",
      "Iteration 56000 with error = 0.0\n",
      "Iteration 57000 with error = 0.0\n",
      "Iteration 58000 with error = 0.0\n",
      "Iteration 59000 with error = 0.0\n",
      "Iteration 60000 with error = 0.0\n",
      "Iteration 61000 with error = 0.0\n",
      "Iteration 62000 with error = 0.0\n",
      "Iteration 63000 with error = 0.0\n",
      "Iteration 64000 with error = 0.0\n",
      "Iteration 65000 with error = 0.0\n",
      "Iteration 66000 with error = 0.0\n",
      "Iteration 67000 with error = 0.0\n",
      "Iteration 68000 with error = 0.0\n",
      "Iteration 69000 with error = 0.0\n",
      "Iteration 70000 with error = 0.0\n",
      "Iteration 71000 with error = 0.0\n",
      "Iteration 72000 with error = 0.0\n",
      "Iteration 73000 with error = 0.0\n",
      "Iteration 74000 with error = 0.0\n",
      "Iteration 75000 with error = 0.0\n",
      "Iteration 76000 with error = 0.0\n",
      "Iteration 77000 with error = 0.0\n",
      "Iteration 78000 with error = 0.0\n",
      "Iteration 79000 with error = 0.0\n",
      "Iteration 80000 with error = 0.0\n",
      "Iteration 81000 with error = 0.0\n",
      "Iteration 82000 with error = 0.0\n",
      "Iteration 83000 with error = 0.0\n",
      "Iteration 84000 with error = 0.0\n",
      "Iteration 85000 with error = 0.0\n",
      "Iteration 86000 with error = 0.0\n",
      "Iteration 87000 with error = 0.0\n",
      "Iteration 88000 with error = 0.0\n",
      "Iteration 89000 with error = 0.0\n",
      "Iteration 90000 with error = 0.0\n",
      "Iteration 91000 with error = 0.0\n",
      "Iteration 92000 with error = 0.0\n",
      "Iteration 93000 with error = 0.0\n",
      "Iteration 94000 with error = 0.0\n",
      "Iteration 95000 with error = 0.0\n",
      "Iteration 96000 with error = 0.0\n",
      "Iteration 97000 with error = 0.0\n",
      "Iteration 98000 with error = 0.0\n",
      "Iteration 99000 with error = 0.0\n"
     ]
    }
   ],
   "source": [
    "# XOR function (one or the other but not both)\n",
    "X = [[0,0], [0,1], [1,0], [1,1]]\n",
    "y = [0, 1, 1, 0]\n",
    "\n",
    "# Init the parameters for the network\n",
    "clf = MultiLayerPerceptron(learning_rate = 0.1, num_iteration = 100000)\n",
    "# Create the architecture backward\n",
    "clf.add_output_layer(num_neuron = 1)\n",
    "clf.add_hidden_layer(num_neuron = 3)\n",
    "clf.add_input_layer(num_neuron = 2)\n",
    "# Train the network\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected 0.0, got:  0.0\n",
      "Expected 1.0, got:  1.0\n",
      "Expected 1.0, got:  1.0\n",
      "Expected 0.0, got:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Expected 0.0, got: \",clf.predict([0,0]))\n",
    "print(\"Expected 1.0, got: \",clf.predict([0,1]))\n",
    "print(\"Expected 1.0, got: \",clf.predict([1,0]))\n",
    "print(\"Expected 0.0, got: \",clf.predict([1,1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
