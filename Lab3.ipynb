{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define parameters\n",
    "num_states = 6  # Number of unique states\n",
    "sequence_length = 100  # Length of random sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "random_states = np.random.randint(0, num_states, size=sequence_length)\n",
    "\n",
    "# Create transition pairs (X, Y)\n",
    "X_train = random_states[:-1]  # Current state\n",
    "y_train = random_states[1:]   # Next state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding function\n",
    "def one_hot_encode(index, num_states):\n",
    "    vec = np.zeros(num_states)\n",
    "    vec[index] = 1\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X_train and y_train to one-hot vectors\n",
    "X_train_oh = np.array([one_hot_encode(idx, num_states) for idx in X_train])\n",
    "y_train_oh = np.array([one_hot_encode(idx, num_states) for idx in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Model parameters\n",
    "input_dim = num_states  # One-hot vector size\n",
    "hidden_dim = 10\n",
    "output_dim = num_states  # Predicting one of the states\n",
    "\n",
    "# Initialize weights and biases\n",
    "np.random.seed(42)\n",
    "W1 = np.random.randn(input_dim, hidden_dim) * 0.01\n",
    "b1 = np.zeros((1, hidden_dim))\n",
    "W2 = np.random.randn(hidden_dim, output_dim) * 0.01\n",
    "b2 = np.zeros((1, output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function (ReLU)\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "# Softmax function for probability distribution\n",
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.7918\n",
      "Epoch 100, Loss: 1.7810\n",
      "Epoch 200, Loss: 1.7734\n",
      "Epoch 300, Loss: 1.7679\n",
      "Epoch 400, Loss: 1.7640\n",
      "Epoch 500, Loss: 1.7611\n",
      "Epoch 600, Loss: 1.7591\n",
      "Epoch 700, Loss: 1.7577\n",
      "Epoch 800, Loss: 1.7566\n",
      "Epoch 900, Loss: 1.7558\n",
      "Epoch 1000, Loss: 1.7553\n",
      "Epoch 1100, Loss: 1.7548\n",
      "Epoch 1200, Loss: 1.7545\n",
      "Epoch 1300, Loss: 1.7543\n",
      "Epoch 1400, Loss: 1.7541\n",
      "Epoch 1500, Loss: 1.7539\n",
      "Epoch 1600, Loss: 1.7538\n",
      "Epoch 1700, Loss: 1.7537\n",
      "Epoch 1800, Loss: 1.7536\n",
      "Epoch 1900, Loss: 1.7536\n",
      "Epoch 2000, Loss: 1.7535\n",
      "Epoch 2100, Loss: 1.7535\n",
      "Epoch 2200, Loss: 1.7534\n",
      "Epoch 2300, Loss: 1.7533\n",
      "Epoch 2400, Loss: 1.7533\n",
      "Epoch 2500, Loss: 1.7532\n",
      "Epoch 2600, Loss: 1.7532\n",
      "Epoch 2700, Loss: 1.7531\n",
      "Epoch 2800, Loss: 1.7530\n",
      "Epoch 2900, Loss: 1.7529\n",
      "Epoch 3000, Loss: 1.7529\n",
      "Epoch 3100, Loss: 1.7528\n",
      "Epoch 3200, Loss: 1.7527\n",
      "Epoch 3300, Loss: 1.7525\n",
      "Epoch 3400, Loss: 1.7524\n",
      "Epoch 3500, Loss: 1.7523\n",
      "Epoch 3600, Loss: 1.7521\n",
      "Epoch 3700, Loss: 1.7519\n",
      "Epoch 3800, Loss: 1.7517\n",
      "Epoch 3900, Loss: 1.7515\n",
      "Epoch 4000, Loss: 1.7513\n",
      "Epoch 4100, Loss: 1.7510\n",
      "Epoch 4200, Loss: 1.7507\n",
      "Epoch 4300, Loss: 1.7504\n",
      "Epoch 4400, Loss: 1.7500\n",
      "Epoch 4500, Loss: 1.7496\n",
      "Epoch 4600, Loss: 1.7491\n",
      "Epoch 4700, Loss: 1.7486\n",
      "Epoch 4800, Loss: 1.7481\n",
      "Epoch 4900, Loss: 1.7475\n",
      "Epoch 5000, Loss: 1.7469\n",
      "Epoch 5100, Loss: 1.7461\n",
      "Epoch 5200, Loss: 1.7454\n",
      "Epoch 5300, Loss: 1.7445\n",
      "Epoch 5400, Loss: 1.7436\n",
      "Epoch 5500, Loss: 1.7426\n",
      "Epoch 5600, Loss: 1.7415\n",
      "Epoch 5700, Loss: 1.7403\n",
      "Epoch 5800, Loss: 1.7390\n",
      "Epoch 5900, Loss: 1.7376\n",
      "Epoch 6000, Loss: 1.7362\n",
      "Epoch 6100, Loss: 1.7346\n",
      "Epoch 6200, Loss: 1.7329\n",
      "Epoch 6300, Loss: 1.7312\n",
      "Epoch 6400, Loss: 1.7293\n",
      "Epoch 6500, Loss: 1.7274\n",
      "Epoch 6600, Loss: 1.7253\n",
      "Epoch 6700, Loss: 1.7232\n",
      "Epoch 6800, Loss: 1.7211\n",
      "Epoch 6900, Loss: 1.7189\n",
      "Epoch 7000, Loss: 1.7166\n",
      "Epoch 7100, Loss: 1.7143\n",
      "Epoch 7200, Loss: 1.7120\n",
      "Epoch 7300, Loss: 1.7096\n",
      "Epoch 7400, Loss: 1.7073\n",
      "Epoch 7500, Loss: 1.7050\n",
      "Epoch 7600, Loss: 1.7027\n",
      "Epoch 7700, Loss: 1.7004\n",
      "Epoch 7800, Loss: 1.6982\n",
      "Epoch 7900, Loss: 1.6960\n",
      "Epoch 8000, Loss: 1.6938\n",
      "Epoch 8100, Loss: 1.6917\n",
      "Epoch 8200, Loss: 1.6896\n",
      "Epoch 8300, Loss: 1.6876\n",
      "Epoch 8400, Loss: 1.6857\n",
      "Epoch 8500, Loss: 1.6838\n",
      "Epoch 8600, Loss: 1.6820\n",
      "Epoch 8700, Loss: 1.6802\n",
      "Epoch 8800, Loss: 1.6784\n",
      "Epoch 8900, Loss: 1.6767\n",
      "Epoch 9000, Loss: 1.6751\n",
      "Epoch 9100, Loss: 1.6734\n",
      "Epoch 9200, Loss: 1.6718\n",
      "Epoch 9300, Loss: 1.6702\n",
      "Epoch 9400, Loss: 1.6687\n",
      "Epoch 9500, Loss: 1.6671\n",
      "Epoch 9600, Loss: 1.6656\n",
      "Epoch 9700, Loss: 1.6641\n",
      "Epoch 9800, Loss: 1.6627\n",
      "Epoch 9900, Loss: 1.6612\n"
     ]
    }
   ],
   "source": [
    "# Training with Backpropagation\n",
    "learning_rate = 0.01\n",
    "epochs = 10000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass\n",
    "    Z1 = np.dot(X_train_oh, W1) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(A1, W2) + b2\n",
    "    y_pred = softmax(Z2)\n",
    "\n",
    "    # Compute loss (cross-entropy)\n",
    "    loss = -np.sum(y_train_oh * np.log(y_pred + 1e-9)) / len(y_train)\n",
    "\n",
    "    # Backpropagation\n",
    "    dZ2 = y_pred - y_train_oh\n",
    "    dW2 = np.dot(A1.T, dZ2) / len(y_train)\n",
    "    db2 = np.sum(dZ2, axis=0, keepdims=True) / len(y_train)\n",
    "\n",
    "    dA1 = np.dot(dZ2, W2.T)\n",
    "    dZ1 = dA1 * (Z1 > 0)  # Derivative of ReLU\n",
    "    dW1 = np.dot(X_train_oh.T, dZ1) / len(y_train)\n",
    "    db1 = np.sum(dZ1, axis=0, keepdims=True) / len(y_train)\n",
    "\n",
    "    # Update parameters\n",
    "    W1 -= learning_rate * dW1\n",
    "    b1 -= learning_rate * db1\n",
    "    W2 -= learning_rate * dW2\n",
    "    b2 -= learning_rate * db2\n",
    "\n",
    "    # Print loss every 100 epochs\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions:\n",
      "State 0 -> Next likely state: 3\n",
      "State 1 -> Next likely state: 3\n",
      "State 2 -> Next likely state: 4\n",
      "State 3 -> Next likely state: 3\n",
      "State 4 -> Next likely state: 1\n",
      "State 5 -> Next likely state: 5\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the next state\n",
    "def predict_next_state(state):\n",
    "    x = one_hot_encode(state, num_states).reshape(1, -1)\n",
    "    z1 = np.dot(x, W1) + b1\n",
    "    a1 = relu(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    y_pred = softmax(z2)\n",
    "    next_state_idx = np.argmax(y_pred)\n",
    "    return next_state_idx\n",
    "\n",
    "# Test the model by predicting the next state\n",
    "print(\"\\nPredictions:\")\n",
    "for state in range(num_states):\n",
    "    next_state = predict_next_state(state)\n",
    "    print(f\"State {state} -> Next likely state: {next_state}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
